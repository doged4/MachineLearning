{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. If you haven't already installed Python and Jupyter Notebook:   \n",
    "    1. Get Python3 from [Python.org](https://www.python.org/downloads/). **Tensorflow does not yet work with Python 3.7, so you _must_ get Python 3.6.** See https://github.com/tensorflow/tensorflow/issues/20517 for updates on 3.7 support.\n",
    "    1. In Terminal, run `python3 -m pip install jupyter`\n",
    "    1. In Terminal, cd to the folder in which you downloaded this file and run `jupyter notebook`. This should open up a page in your web browser that shows all of the files in the current directory, so that you can open this file. You will need to leave this Terminal window up and running and use a different one for the rest of the instructions.\n",
    "1. Install the Gensim word2vec Python implementation: `pip3 install --upgrade gensim`\n",
    "1. Get the trained model (1billion_word_vectors.zip) from me via airdrop or flashdrive and put it in the same folder as the ipynb file, the folder in which you are running the jupyter notebook command.\n",
    "1. Unzip the trained model file. You should now have three files in the folder (if zip created a new folder, move these files out of that separate folder into the same folder as the ipynb file):\n",
    "    * 1billion_word_vectors\n",
    "    * 1billion_word_vectors.syn1neg.npy\n",
    "    * 1billion_word_vectors.wv.syn0.npy\n",
    "1. If you didn't install keras last time, install it now\n",
    "    1. Install the tensorflow machine learning library by typing the following into Terminal:\n",
    "    `pip3 install --upgrade tensorflow`\n",
    "    1. Install the keras machine learning library by typing the following into Terminal:\n",
    "    `pip3 install keras`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Details -- Do Not Do This\n",
    "This took awhile, which is why I'm giving you the trained file rather than having you do this. But just in case you're curious, here is how to create the trained model file.\n",
    "1. Download the corpus of sentences from [http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz)\n",
    "1. Unzip and unarchive the file: `tar zxf 1-billion-word-language-modeling-benchmark-r13output.tar.gz` \n",
    "1. Run the following Python code:\n",
    "    ```\n",
    "    from gensim.models import word2vec\n",
    "    import os\n",
    "\n",
    "    corpus_dir = '1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled'\n",
    "    sentences = word2vec.PathLineSentences(corpus_dir)\n",
    "    model = word2vec.Word2Vec(sentences) # just use all of the default settings for now\n",
    "    model.save('1billion_word_vectors')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation/Sources\n",
    "* [https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html) for more information about how to use gensim word2vec in general\n",
    "* [https://codekansas.github.io/blog/2016/gensim.html](https://codekansas.github.io/blog/2016/gensim.html) for information about using it to create embedding layers for neural networks.\n",
    "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
    "* [https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) for using pre-trained embeddings with keras (though the syntax they use for the model layers is different than most other tutorials I've seen).\n",
    "* [https://keras.io/](https://keras.io/) Keras API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model file into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = word2vec.Word2Vec.load('1billion_word_vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not need to continue training the model, we can save memory by keeping the parts we need (the word vectors themselves) and getting rid of the rest of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = wv_model.wv\n",
    "del wv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of word vectors\n",
    "Now we can look at some of the relationships between different words.\n",
    "\n",
    "Like [the gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html), let's start with a famous example: king + woman - man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8407387733459473),\n",
       " ('monarch', 0.7541723251342773),\n",
       " ('prince', 0.7350203394889832),\n",
       " ('princess', 0.696908175945282),\n",
       " ('empress', 0.677180290222168),\n",
       " ('sultan', 0.6649758815765381),\n",
       " ('Chakri', 0.6451102495193481),\n",
       " ('goddess', 0.6439394950866699),\n",
       " ('ruler', 0.6275453567504883),\n",
       " ('kings', 0.6273428201675415)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one does not work as well as I'd hoped, but it gets close. Maybe you can find a better example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('okapi', 0.7140712738037109),\n",
       " ('gibbon', 0.7034620046615601),\n",
       " ('koala', 0.697202742099762),\n",
       " ('cub', 0.6907659769058228),\n",
       " ('tortoise', 0.6886162757873535),\n",
       " ('beetle', 0.6859476566314697),\n",
       " ('salamander', 0.6855185031890869),\n",
       " ('psyllid', 0.6837549209594727),\n",
       " ('lynx', 0.6802860498428345),\n",
       " ('carnivore', 0.6794542670249939)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.most_similar(positive=['panda', 'eucalyptus'], negative=['bamboo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imdb_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-80a52aa43336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimdb_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'thing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'imdb_map' is not defined"
     ]
    }
   ],
   "source": [
    "imdb_map['thing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one of these is not like the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.doesnt_match(['red', 'purple', 'laptop', 'turquoise', 'ruby'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How far apart are different words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.distances('laptop', ['computer', 'phone', 'rabbit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what one of these vectors actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec['textbook']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.most_similar(positive=['man', 'success'], negative=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.most_similar(positive=['Beijing', 'Japan'], negative=['China'])\n",
    "wordvec.most_similar(positive=['Beijing'], negative=[])\n",
    "wordvec.most_similar(positive=['Quebec', 'States'], negative=['Canada'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.most_similar(positive=['tree', 'human'], negative=['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec.most_similar(positive=['panda', 'eucalyptus'], negative=['bamboo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.205414  , 0.36557418, 0.6597437 ], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.distances('laptop', ['computer', 'phone', 'rabbit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37990654, 0.22643244, 0.39046144], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.distances('shark', ['tuna', 'turtle', 'salmon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordvec['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other methods are available to us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the word vectors in an embedding layer of a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification without using the pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition. The embedding layer here learns the 100-dimensional vector embedding within the overall classification problem training. That is usually what we want, unless we have a bunch of un-tagged data that could be used to train word vectors but not classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. __This takes awhile. You might not want to re-run it.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For any model that you try in these exercises, take notes about the performance you see and anything you notice about the differences between the models.\n",
    "\n",
    "# Exercise Option 1: Use the word vectors in a full model\n",
    "Using the knowledge about how the imdb dataset and the keras embedding layer represent words, as detailed above, define a model that uses the pre-trained word vectors from the imdb dataset rather than an embedding that keras learns as it goes along. You'll need to swap out the embedding layer and feed in different training data.\n",
    "\n",
    "# Exercise Option 2:\n",
    "Same as option 1, but try using the 1billion vector word embeddings instead of the imdb vectors.\n",
    "\n",
    "# Exercise Option 3:\n",
    "Try changing different hyperparameters of the not_pretrained model. Keep notes on how the performance changes.\n",
    "\n",
    "# Exercise Option 4: From here:\n",
    "Make a model for the reuters classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(r_x_train, raw_r_y_train), (r_x_test, raw_r_y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=500,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review help here: https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "dataEnc = OneHotEncoder(sparse=False)\n",
    "r_y_train =dataEnc.fit_transform(raw_r_y_train.reshape(-1,1))\n",
    "dataEnc2 = OneHotEncoder(sparse=False)\n",
    "r_y_test =dataEnc2.fit_transform(raw_r_y_test.reshape(-1,1))\n",
    "\n",
    "#for i in range(0,len(t)):\n",
    "#    print(raw_r_y_train[i])\n",
    "#    print(r_y_train[i])\n",
    "#yay it one hot encode properly!\n",
    "type(r_y_test[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "r_x_train_padded = sequence.pad_sequences(r_x_train, maxlen=500)\n",
    "r_x_test_padded = sequence.pad_sequences(r_x_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_offset = 3\n",
    "reuters_map = dict((index + reuters_offset, word) for (word, index) in reuters.get_word_index().items())\n",
    "reuters_map[0] = 'PADDING'\n",
    "reuters_map[1] = 'START'\n",
    "reuters_map[2] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuterbasic_model = Sequential()\n",
    "first_embedding_layer = wordvec.get_keras_embedding()\n",
    "first_embedding_layer.input_length = 500 #number of words input\n",
    "first_embedding_layer.input_dim = 552402 #reuters data\n",
    "first_embedding_layer.output_dim = 100 #number of dimensions of outputted wordvec\n",
    "reuterbasic_model.add(first_embedding_layer)\n",
    "reuterbasic_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuterbasic_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuterbasic_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuterbasic_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuterbasic_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuterbasic_model.add(Flatten())\n",
    "reuterbasic_model.add(Dense(units=128, activation='relu'))\n",
    "reuterbasic_model.add(Dense(units=46, activation='softmax')) # categorical\n",
    "reuterbasic_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8621/8621 [==============================] - 27s 3ms/step - loss: 2.2418 - categorical_accuracy: 0.4249\n",
      "Epoch 2/5\n",
      "8621/8621 [==============================] - 30s 3ms/step - loss: 1.8937 - categorical_accuracy: 0.5169\n",
      "Epoch 3/5\n",
      "8621/8621 [==============================] - 33s 4ms/step - loss: 1.6335 - categorical_accuracy: 0.5752\n",
      "Epoch 4/5\n",
      "8621/8621 [==============================] - 27s 3ms/step - loss: 1.2300 - categorical_accuracy: 0.6785\n",
      "Epoch 5/5\n",
      "8621/8621 [==============================] - 27s 3ms/step - loss: 0.8649 - categorical_accuracy: 0.7706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12cafb748>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuterbasic_model.fit(r_x_train_padded, r_y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2156/2156 [==============================] - 3s 1ms/step\n",
      "loss: 2.093535653293022 accuracy: 0.5408163265306123\n"
     ]
    }
   ],
   "source": [
    "reuter_scoring = reuterbasic_model.evaluate(r_x_test_padded, r_y_test)\n",
    "print('loss: {} accuracy: {}'.format(*reuter_scoring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add kfold next to see..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
